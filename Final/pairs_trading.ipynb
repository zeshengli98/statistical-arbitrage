{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248fe069",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pairs Trading Strategy Design & Backtest\n",
    "\n",
    "The definition of Pairs Trading is to take advantage of the mispricing between two or more assets by taking long and short portfolio, betting the related movement will converge back when mispricing situation happening. The principle is buying undervalued and selling overvalued. This is a very charming strategy that we can extend the idea in any market of interest. \n",
    "\n",
    "The first pairs trading bears its roots in equity market, but we can possibly extend the idea into commodities, cryptos, and even options market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5127c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import required lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d2bf64",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import pyfolio as pf\n",
    "import yfinance as yf\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaa53d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pairs Trading Tutourial\n",
    "## Signal Generation and Backtesting\n",
    "- Be inventive beyond equity pairs: consider commodity futures, instruments on interest rates, and aggregated indices.\n",
    "- Arb is relized by using cointegrating $\\beta_{Coint}$ as allocation weight. All project designs should including trading signal generation(OU process fitting) and backtesting.\n",
    "- Does P&L behave as expected for cointegration arb trade? Is P&L coming from a few or many trades, what is half-life? Maximum Drawdown and behaviour of volatility/VaR?\n",
    "- Introduce liquidity and algorithmic flow considerations (a model of order flow). Any rules on accumulating the position? What impact bid-ask spread and transaction costs will make?\n",
    "## Step-by-step instructions\n",
    "- Part I: pairs trading design\n",
    "    - 1. re-code regression estimation in martrix form - your own OLS implementation which you can re-use. Regression between stationary variables(DF test regression/difference equation) has optional model specification test for (a)identifying optimal lag p with AIC BIC tests and (b) stability check\n",
    "    - 2. Implement Engle-Granger procedure for each pair. Step1 use ADF test for unit root with lag1. Step2, formulate both correction equation and decide which one is more important\n",
    "    - 3. Decide signals: $\\mu_e\\pm Z\\sigma_{eq}$ and exit on $\\mu_t$\n",
    "    - 4. At first aussme Z=1. Then change Z sightly upwards and downwards - compute P&L for each case of bounds. Alternatively run an optimization that varies Z and maxmize the P&L or other criterion.\n",
    "    - 5. Optionally us VECM in order select the best candidate for pairs trading (or basket trading).\n",
    "    \n",
    "- Part II: Backtesting\n",
    "    - 1. perform systematic backtesting of your trading strategy platform to produce drawdown plots, rolling Sharpe ratio and rolling beta\n",
    "    - 2. keep delivering staionary spread over 3-6 months. Kalman filter will give updated beta. However, you can simply re-estmate cointergration by shifting data 1-2 weeks and report beta and EG.\n",
    "    - 3. use Machine-learning-inspired backtesting, such as spliting data, time series CV.\n",
    "    \n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part I: Pairs Trading Design\n",
    "## 1.1 Data Processing\n",
    "One study by Jacob & Weber conducted several international markets which has empirically proven that the pairs trading works the most in emerging market, either from the high ineffciencies or a large number of available pairs. So I believe some innovative market has more opportunities than equity market.\n",
    "\n",
    "In this case, I want to study model-driven statistical arbitrage strategies in commodities and crypto market. The crpyto is the youngest and has less research than other assets, which becomes very attractive for pairs trading strategy design. From those three different perspectives, we can identify multiple strategy implementations and more profitable opportunities. So, in the first step, we sort out a list of available symbols and prepare them for filtering."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## In this case, I want to implement pairs trading from two perspectives - equity and commodity market.\n",
    "## First I listed some potentially profitable tickers to be tested from different market.\n",
    "start = '2005-01-01'\n",
    "end = '2022-07-30'\n",
    "tickers_commodity = {'Gold':'GC=F',\n",
    "                     'Sliver':'SI=F',\n",
    "                     'Crude Oil':'CL=F',\n",
    "                     'Natural Gas':'NG=F',\n",
    "                     'Gasoline':'RB=F',\n",
    "                     'E-Mini S&P 500':'ES=F'}\n",
    "\n",
    "tickers_crypto = ['BTC-USD','ETH-USD','ADA-USD','SOL-USD','BNB-USD','DOGE-USD','LTC-USD']\n",
    "\n",
    "tickers_stock = ['']\n",
    "price_commodity = yf.download(list(tickers_commodity.values()), start, end)['Adj Close'].dropna()\n",
    "price_commodity.rename({v:k for k,v in tickers_commodity.items()},axis=1,inplace=True)\n",
    "\n",
    "price_crypto = yf.download(tickers_crypto, '2017-12-01', end)['Adj Close'].dropna()['2021-07':]\n",
    "price_crypto.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "price_commodity.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Cointegration Apporach\n",
    "Cointegration: I(d) series, which means integrated series of order d\n",
    "I(1) series: Price\n",
    "I(0) series: Returns\n",
    "The prices of cointegrated assets fluctuate around a certain average level. So cointegration allows us to construct a 2-asset portfolio with stationary series to be traded. Then we are able to construct a mean-reversion strategy.\n",
    "\n",
    "### Find $\\beta_{Coint}$\n",
    "- Engle-Grange test\n",
    "    - Linear regression on the candidate pairs price and calculate its residual\n",
    "    - Test the stationary of the residual\n",
    "- Johansen test\n",
    "    - VECM\n",
    "    \n",
    "    \n",
    "We have two apporaches to find cointegration beta parameter. \n",
    "### Engle-Grange\n",
    "The first idea of the Engle-Granger test is to perform a linear regression between two underlying assets and test its residual, and see if the series is stationary by applying the Augmented Dick-Fuller test. So if the residual is a stationary series, we can say the two prices are cointegrated. The $\\beta_{Coint}$ is obtained as the asset weight to be traded.\n",
    "\n",
    "\n",
    "In the stationarity test, we test for a unit root, which is based on the following hypothesis test:\n",
    "$$H_0: \\phi =1 \\rightarrow y_t \\sim I(0) | (unit root)$$\n",
    "$$H_1:\\mid\\phi\\mid <1 \\rightarrow y_t \\sim I(0) | (stationary)$$\n",
    "\n",
    "### ADF test\n",
    "**ADF test equation**\n",
    "use ADF test for unit root with lag1:\n",
    "$$\\Delta e_t = \\varphi e_{t-1} + \\varphi_{aug1}\\Delta e_{t-1} + const + \\varphi_tt+\\epsilon_t$$\n",
    "- Improvement 1. Test equation above includes time dependence  $\\varphi_tt$ , referred to as 'trend'.\n",
    "I don't include trend in the ADF tests and cointegrating residual -- it will make me think cointegration is present when it is very weak. In fact, without $\\varphi_tt$ term, we might not even get stationarity result."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def OLS(y, x):\n",
    "    '''\n",
    "    parameters:\n",
    "    :param y: independent variable, dataframe or array-like\n",
    "    :param x: dependent variables, dataframe or array-like\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "\n",
    "    model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "\n",
    "    residuals = model.resid\n",
    "    residuals = pd.DataFrame({'resid':residuals},index = x.index)\n",
    "    ## OLS params\n",
    "    c, beta = model.params\n",
    "\n",
    "    ## OLS params sd\n",
    "    c_sd, beta_sd = model.bse\n",
    "\n",
    "    # OLS t-statistics\n",
    "    c_t, beta_t = model.tvalues\n",
    "    \n",
    "    \n",
    "    summary = pd.DataFrame({\"Params\":model.params,\n",
    "                       \"Error\":model.bse,\n",
    "                       'T-stats':model.tvalues,\n",
    "                       'P-values': model.pvalues})\n",
    "    \n",
    "    return beta, residuals, np.around(summary,2)\n",
    "\n",
    "def ADF_test(resid, name, verbose=True):\n",
    "    # H_0 in adfuller is unit root exists (non-stationary)\n",
    "    # We must observe significant p-value to convince ourselves that the series is stationary\n",
    "    '''\n",
    "    :param resid: dataframe-like, the residual from OLS or any other series to be tested\n",
    "    '''\n",
    "    index = resid.index\n",
    "    resid = np.array(resid).flatten()\n",
    "    series = pd.DataFrame({'e_t':resid},index = index)\n",
    "    series['e_t-1'] = series['e_t'].shift(1)\n",
    "    series['Δe_t'] = series['e_t'].diff()\n",
    "    series['Δe_t-1'] = series['e_t'].diff().shift(1)\n",
    "    \n",
    "    series = series.dropna()\n",
    "    x = series[['e_t-1','Δe_t-1']]\n",
    "    y = series['Δe_t']\n",
    "    \n",
    "    model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "\n",
    "    summary = pd.DataFrame({f\"Estimate ΔRes({name})\":model.params,\n",
    "                           f\"SD of Estimate ΔRes({name})\":model.bse,\n",
    "                           f\"t-Statistic ΔRes({name})\":model.tvalues,\n",
    "                           f\"P-value ΔRes({name})\":model.pvalues})\n",
    "    summary = np.around(summary,6)\n",
    "#     display(summary)\n",
    "\n",
    "    adf = adfuller(series['e_t'],regression='c')\n",
    "    aic = adf[-1]\n",
    "\n",
    "    pvalue = round(adf[1],6)\n",
    "    if verbose==True:\n",
    "        print(\"---------ADF result--------:\")\n",
    "        if pvalue < 0.05:\n",
    "            print('p-value = ' + str(pvalue) + ' The series ' + name +' is likely stationary.')\n",
    "        else:\n",
    "            print('p-value = ' + str(pvalue) + ' The series ' + name +' is likely non-stationary.')\n",
    "    \n",
    "    return summary, pvalue, aic\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Pair Candidates Selection\n",
    "Before we do cointegration, we have two baskets of assets - commodities and cryptocurrencies. We will only look for cointegrating pairs in the basket because the ADF test is not good at identifying spurious relationships."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pairs_selection(prices):\n",
    "    '''\n",
    "    :param prices: dataframe-like, all asset prices to be checked to find candidate pairs\n",
    "    '''\n",
    "    res = pd.DataFrame(columns = ['p-value','aic'])\n",
    "    n = prices.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            pairs = prices.iloc[:,[i,j]]\n",
    "#             print(pairs)\n",
    "            pairs_name = f\"({pairs.columns[0]}, {pairs.columns[1]})\"\n",
    "            y = pairs.iloc[:,0]\n",
    "            x = pairs.iloc[:,1]\n",
    "            beta, resid, summary = OLS(y, x)\n",
    "            \n",
    "            summary, p, aic = ADF_test(resid, pairs_name, verbose=False)\n",
    "            res.loc[pairs_name] = [p, aic]\n",
    "    res.sort_values(by=['p-value'],inplace=True)\n",
    "    return res\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Filtering of Commodity Basket\n",
    "By calculating the ADF p-values for all pairs, we find that crude oil and gasoline have the most significant cointegration relationship, which is very reasonable because the properties of these two commodities are very close and crude oil is the raw material for gasoline.\n",
    "\n",
    "So in the commodity basekt, we will further study this candidate pair."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pairs_selection(price_commodity).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_commodity = price_commodity.loc[:,'Crude Oil']\n",
    "x_commodity = price_commodity.loc[:,'Gasoline']\n",
    "\n",
    "beta_commodity, resid_commodity, summary_commodity = OLS(y_commodity, x_commodity)\n",
    "# resid.plot(figsize=(10,8))\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot(resid_commodity)\n",
    "ax.set_title('Res(Crude Oil, Gasoline)')\n",
    "ax.set_xlabel('Date')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Filtering of Crypto Basket\n",
    "The cryptocurrency market is very volatile and we cannot be sure that these pairs have all time covariance, so we have to look forward a year and find the relationship over that time.\n",
    "\n",
    "Fortunately, we found that the BNB-ETH pair was a possible cointegrated pair in the previous year, so we can assume that this relationship will be maintained in the following short period of time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pairs_selection(price_crypto).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_crypto = price_crypto.loc[:,'BNB-USD']\n",
    "x_crypto = price_crypto.loc[:,'ETH-USD']\n",
    "\n",
    "beta_crypto, resid_crypto, summary_crypto = OLS(y_crypto, x_crypto)\n",
    "# resid.plot(figsize=(10,8))\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot(resid_crypto)\n",
    "ax.set_title('Res(BNB, ETH)')\n",
    "ax.set_xlabel('Date')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Cointegration Analysis\n",
    "The results of cointegration tests reveal situations in which two or more non-stationary time series are combined in such a way that they are unable to deviate from equilibrium over the long run. The tests help determine how sensitive two variables are to the same average price over a certain time period.\n",
    "### 1.4.1 ENGLE-GRANGER STEP-1. Cointegrated Residual\n",
    "We can perform the 1st step of EG process which is to estimate the model from OLS:\n",
    "$$y_t = \\beta_{Coint}*x_t + \\mu_e + \\epsilon_t$$\n",
    "Then our naive cointegrated residual is:\n",
    "- Commodity pair\n",
    "$$e_t = Crude Oil_t - 33.48*Gasoline_t - 1.33$$\n",
    "- Crypto pair\n",
    "$$e_t = BNB_t - 0.11*ETH_t - 87.76$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary_commodity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary_crypto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary, p, aic = ADF_test(resid_commodity, 'Crude Oil, Gasoline')\n",
    "display(summary)\n",
    "summary, p, aic = ADF_test(resid_crypto, 'BNB, ETH') \n",
    "display(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both pairs passed the ADF test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### ENGLE-GRANGER STEP 2. Error correction equations\n",
    "In general form, \n",
    "$$\\Delta P_t^A = \\varphi \\Delta P_t^B - (1-\\alpha) \\tilde e^A_{t-1} + \\epsilon_t$$\n",
    "\n",
    "In the other way around:\n",
    "$$\\Delta P_t^B = \\varphi \\Delta P_t^A - (1-\\alpha) \\tilde e^A_{t-1} + \\epsilon_t$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def EG_err_corr1(S1, S2, e, name):\n",
    "    '''\n",
    "    :param S1: dataframe-like, the price of asset A\n",
    "    :param S2: dataframe-like, the price of asset B\n",
    "    :param e: dataframe-like, the residual between A and B\n",
    "    '''\n",
    "    dS1 = np.array(S1.diff()).flatten()\n",
    "    dS2 = np.array(S2.diff()).flatten()\n",
    "    \n",
    "    e = np.array(e.shift(1)).flatten()\n",
    "    df_to_fit = pd.DataFrame({'ΔPA_t':dS1,\n",
    "                             'ΔPB_t':dS2,\n",
    "                             'e_t-1':e}).dropna()\n",
    "    \n",
    "    x = df_to_fit[['ΔPB_t','e_t-1']]\n",
    "    y = df_to_fit['ΔPA_t']\n",
    "    \n",
    "    model = sm.OLS(y, x).fit()\n",
    "\n",
    "    summary = pd.DataFrame({f\"Estimate Δ{name}\":model.params,\n",
    "                           f\"SD of Estimate Δ{name}\":model.bse,\n",
    "                           f\"t-Statistic Δ{name}\":model.tvalues,\n",
    "                           f\"P-value Δ{name}\":model.pvalues})\n",
    "    summary = np.round(summary,6)\n",
    "    return summary, model.pvalues\n",
    "summary, p = EG_err_corr1(y_commodity, x_commodity, resid_commodity, 'Gasoline')\n",
    "display(summary)\n",
    "summary, p = EG_err_corr1(y_crypto, x_crypto, resid_crypto, 'ETH')\n",
    "display(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now perform **the 2nd step of EG to estimate the Equilibrium Correction Model**, Here we check the significance of -(1-𝛼) term, which ensure the correction the long run equilibrium. In error correction equation the p-value is significantly showing 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def EG_err_corr2(S1, S2, e, name):\n",
    "    '''\n",
    "    :param S1: dataframe-like, the price of asset A\n",
    "    :param S2: dataframe-like, the price of asset B\n",
    "    :param e: dataframe-like, the residual between A and B\n",
    "    '''\n",
    "    dS1 = np.array(S1.diff()).flatten()\n",
    "    dS2 = np.array(S2.diff()).flatten()\n",
    "    \n",
    "    e = np.array(e.shift(1)).flatten()\n",
    "    df_to_fit = pd.DataFrame({'ΔPA_t':dS1,\n",
    "                             'ΔPB_t':dS2,\n",
    "                             'e_t-1':e}).dropna()\n",
    "    \n",
    "    x = df_to_fit[['ΔPA_t','e_t-1']]\n",
    "    y = df_to_fit['ΔPB_t']\n",
    "    \n",
    "    model = sm.OLS(y, x).fit()\n",
    "\n",
    "    summary = pd.DataFrame({f\"Estimate Δ{name}\":model.params,\n",
    "                           f\"SD of Estimate Δ{name}\":model.bse,\n",
    "                           f\"t-Statistic Δ{name}\":model.tvalues,\n",
    "                           f\"P-value Δ{name}\":model.pvalues})\n",
    "    summary = np.round(summary,6)\n",
    "    \n",
    "    return summary, model.pvalues\n",
    "summary, p = EG_err_corr2(y_commodity, x_commodity, resid_commodity, 'Crude Oil')\n",
    "display(summary)\n",
    "summary, p = EG_err_corr2(y_crypto, x_crypto, resid_crypto, 'BNB')\n",
    "display(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we estimate the EG correction equation \"other way around\", both shows the significance.\n",
    "From the absolute value of t-stas, the first way is the more important model. So the pair is considered to be cointegrated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1.5 Ornstein-Uhlenbeck process\n",
    "\n",
    "In order to find the optimal $\\beta_{Coint}$ to build the best mean reversion portfolio, we can fit the OU process.\n",
    " \n",
    "The Ornstein-Uhlenbeck process is described by the following SDE:\n",
    "\n",
    "$$ dX_t = \\kappa (\\theta - X_t) dt + \\sigma dW_t $$\n",
    "\n",
    "The parameters are:\n",
    "- $\\theta \\in \\mathbb{R}$: The long-term average, around which all trajectories of $X_t$ oscillate, is the mean level.\n",
    "\n",
    "- $\\kappa > 0$: the speed of mean reversion, represents the velocity at which such trajectories will regroup around mean level\n",
    "\n",
    "- $\\sigma > 0$: instantaneous volatility, measures the amplitude of randomness entering the system.\n",
    "\n",
    "\n",
    "At this point we can solve the SDE:\n",
    "\n",
    "$$ X_t = \\theta + (X_0 - \\theta)e^{-\\kappa t} + \\int_0^t \\sigma\\, e^{\\kappa (s-t)} dW_s .$$\n",
    "\n",
    "\n",
    "### Moments:\n",
    "\n",
    "The **mean** of $X_t$ is:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\mathbb{E}[X_t] &= \\mathbb{E}\\biggl[ \\theta + (X_0 - \\theta)e^{-\\kappa t} + \\int_0^t \\sigma\\, e^{\\kappa (s-t)} dW_s \\biggr] \\\\\n",
    "                &= \\theta + (X_0 - \\theta)e^{-\\kappa t}    \n",
    "\\end{aligned}$$\n",
    "\n",
    "The **covariance** is:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\text{Cov}[X_s, X_t] &= \\frac{\\sigma^2}{2\\kappa} \\biggl( e^{-\\kappa |t-s|} - e^{-\\kappa (s+t)}\\, \\biggr),  \n",
    "\\end{aligned}$$\n",
    "\n",
    "The **variance** is: \n",
    "\n",
    "$$ \\text{Var}[X_t] = \\text{Cov}[X_t, X_t] = \\frac{\\sigma^2}{2\\kappa} \\biggl( 1- e^{-2 \\kappa t} \\biggr).$$\n",
    "\n",
    "So, we can obtain the **mean**: $\\theta$ and the **variance**: $\\frac{\\sigma^2}{2\\kappa}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can discretize the SDE using the Euler-Maruyama numerical method:\n",
    "\n",
    "Let us consider the solution of the OU SDE obtained above. We can compute $X_{n+1}$ and consider the initial value at time $n$.\n",
    "\n",
    "$$ X_{n+1} = \\theta + (X_n - \\theta)e^{-\\kappa \\Delta t} + \\sqrt{\\frac{\\sigma^2}{2\\kappa} \\bigl( 1- e^{-2 \\kappa \\Delta t} \\bigr)} \\; \\epsilon_n $$ \n",
    "\n",
    "with $\\epsilon_n \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "\n",
    "\n",
    "### Estimation of parameters\n",
    "\n",
    "We can compute $X_{t+\\Delta t}$ and consider the initial value at time $t$.\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "X_{t+\\Delta t} &= \\theta + (X_t - \\theta)e^{-\\kappa \\Delta t} + \\int_t^{t+\\Delta t} \\sigma\\, e^{\\kappa (s-t)} dW_s \\\\\n",
    "               &= \\theta \\bigl( 1-e^{-\\kappa \\Delta t} \\bigr) + e^{-\\kappa \\Delta t} X_t + \\int_t^{t+\\Delta t} \\sigma\\, e^{\\kappa (s-t)} dW_s \\\\\n",
    "               &= \\alpha + \\beta X_t + \\epsilon_t\n",
    "\\end{aligned} $$\n",
    "\n",
    "where $\\alpha = \\theta \\bigl( 1-e^{-\\kappa \\Delta t} \\bigr)$, $\\beta = e^{-\\kappa \\Delta t}$ and with $\\epsilon_t \\sim \\mathcal{N}\\biggl( 0, \\frac{\\sigma^2}{2\\kappa} \\bigl( 1- e^{-2 \\kappa \\Delta t} \\bigr)\\biggr)$.\n",
    "\n",
    "So, this confirms the saying from \"The Ornstein–Uhlenbeck process can also be considered as the continuous-time analogue of the discrete-time AR(1) process.\" and we are able to guarantee the AR(1) process to estimate the params on the spread. \n",
    "\n",
    "let us use the usual OLS method to estimate $\\alpha$, $\\beta$ and $\\sigma$.\n",
    "Then, we can obtain the parameters from the formulas:\n",
    "\n",
    "$$ \\kappa = - \\frac{\\log \\beta}{\\Delta t}, \\quad \\theta = \\frac{\\alpha}{1-\\beta}, \\quad \n",
    "\\sigma = \\text{Std}[\\epsilon_t] \\sqrt{ \\frac{2\\kappa}{1-\\beta^2} }$$\n",
    "\n",
    "we can obtain almost consistent parameters to those params obtained by MLE.\n",
    "\n",
    "Halflife:\n",
    "$$Halflife(days) = \\frac{ln(2)}{\\theta*dt}$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OU_process:\n",
    "    def __init__(self, freq = 'D'):\n",
    "        if freq =='D':\n",
    "            self.dt = 1/252\n",
    "        elif freq=='H':\n",
    "            self.dt = 1/252/60\n",
    "        elif freq==\"M\":\n",
    "            self.dt = 1/252/60/60\n",
    "        \n",
    "    def fit(self,resid, verbose = True):\n",
    "        X = np.array(resid[:-1]).flatten()\n",
    "        Y = np.array(resid[1:]).flatten()\n",
    "\n",
    "        model = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "        alpha, beta = model.params\n",
    "        kappa = - np.log(beta)/self.dt\n",
    "        theta = alpha/(1-beta)\n",
    "        res = Y - beta * X - alpha                   # residuals\n",
    "        std_resid = np.std(res, ddof=2)\n",
    "        sigma = std_resid * np.sqrt(2*kappa/(1-beta**2))\n",
    "        sigma_eq = std_resid*np.sqrt(1/(1-beta**2))\n",
    "        halflife = np.log(2)/kappa/self.dt\n",
    "        if verbose:\n",
    "            print(\"OU process params:\")\n",
    "            print(f\"theta = {theta:.4f}\")\n",
    "            print(f\"kappa = {kappa:.4f}\")\n",
    "            print(f\"sigma = {sigma:.4f}\")\n",
    "            print(f\"sigma_eq = {sigma_eq:.4f}\")\n",
    "            print(f\"halflife = {halflife:.4f}\")\n",
    "        self.params = {'theta':theta,'kappa':kappa, 'sigma':sigma,'sigma_eq':sigma_eq,'halflife':halflife}\n",
    "        return self\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5.1 Commodity Basket OU process fitting\n",
    "In this step, I will apply the Ornstein-Uhlenbeck process for modellling the cointegration residual for each pair in the commodity and crypto basket. The parameters can be easily estimated from last funcion. We are interested in the pair with larger kappa so that they are expected to revert to mean level more quickly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def OU_process_pairs_selection(prices, kappa_thres = 5):\n",
    "    res = pd.DataFrame(columns = ['theta','kappa', 'sigma','sigma_eq','halflife'])\n",
    "    resid_df = pd.DataFrame(index = prices.index)\n",
    "    n = prices.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            pairs = prices.iloc[:,[i,j]]\n",
    "            pairs_name = f\"({pairs.columns[0]}, {pairs.columns[1]})\"\n",
    "            y = pairs.iloc[:,0]\n",
    "            x = pairs.iloc[:,1]\n",
    "            beta, resid, summary = OLS(y, x)\n",
    "            \n",
    "            params = OU_process().fit(resid,verbose=False).params\n",
    "            if params['kappa']>kappa_thres:\n",
    "                res.loc[pairs_name] = list(params.values())\n",
    "                resid_df[pairs_name] = resid\n",
    "    res.sort_values(by=['kappa'],ascending=False,inplace=True)\n",
    "    return res, resid_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OU_params_commodity, resid_df = OU_process_pairs_selection(price_commodity)\n",
    "OU_params_commodity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OU_params_crypto, resid_df = OU_process_pairs_selection(price_crypto, kappa_thres=8)\n",
    "OU_params_crypto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From OU process pair selection part, we can conclude that the Crude Oil - Gasoline pair and BNB-ETH pair have high mean reversion rate, which satisfies our expection, so the pairs we selected previously are reliable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 Time series split into train and test set\n",
    "Before we do signal generation, we will apply some machine learning-inspired backtesting, such as spliting data into train/test subsets. We will train the best parameters on the training data set and regard the test set as real-time trading. So it is easier to deploy time series cross validation in the following steps.\n",
    "\n",
    "Considering the halflife of our model, we need to have enough long time period in the test dataset. So we will ensure the cointegration window will contain at least one year and the backtesting window should be more than half year to maintain at least one trade during the subsets.\n",
    "\n",
    "We will seperate 70% time series into training set and remaining 30% into test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_commodity = price_commodity[['Crude Oil','Gasoline']]\n",
    "pair_crypto = price_crypto[['BNB-USD','ETH-USD']]\n",
    "prices_train_commodity, prices_test_commodity= train_test_split(pair_commodity, test_size=0.3,shuffle=False)\n",
    "prices_train_crypto, prices_test_crypto = train_test_split(pair_crypto, test_size=0.3,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 Signal Generation\n",
    "\n",
    "The pair candidates have been selected and their cointegration tests run too. The following process will run into the backtesting phase. Before that, we should generate trading signal series from the prices. First, we must find when a position should be opened and liquidated. Now that we have 𝜃θ - the mean reversion average level - and $\\sigma_{eq}$ from OU process, we are able to standardize the residuals and set the threshold:\n",
    "\n",
    "Let's keep our trading rule as simple as possible, set the Z to 1.\n",
    "- Entry on $\\mu_e\\pm Z\\sigma_{eq}$ \n",
    "    - if > $Z\\sigma$: Go short Y and long X at a ratio of H of X for every dollor of Y\n",
    "- Exit on $\\mu_e$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6.1 Commodity Signal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_commodity = OU_params_commodity.loc['(Crude Oil, Gasoline)']['theta']\n",
    "sig_commodity = OU_params_commodity.loc['(Crude Oil, Gasoline)']['sigma_eq']\n",
    "hl_commodity = OU_params_commodity.loc['(Crude Oil, Gasoline)']['halflife']\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(resid_commodity)\n",
    "ax.axhline(mu_commodity,color = 'orange',label = \"mu\")\n",
    "ax.axhline(mu_commodity+sig_commodity,color = 'red',linestyle ='--',label = \"mu+sigma\")\n",
    "ax.axhline(mu_commodity-sig_commodity,color = 'green',linestyle ='--',label = \"mu-sigma\")\n",
    "ax.axvline(prices_train_commodity.index[-1],color = 'black',linestyle ='--',label = \"Time series split\")\n",
    "ax.set_title(\"(Crude Oil, Gasoline) Trading Signal\")\n",
    "ax.legend(loc='lower left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6.2 Crypto Signal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_crypto = OU_params_crypto.loc['(BNB-USD, ETH-USD)']['theta']\n",
    "sig_crypto = OU_params_crypto.loc['(BNB-USD, ETH-USD)']['sigma_eq']\n",
    "hl_crypto = OU_params_crypto.loc['(BNB-USD, ETH-USD)']['halflife']\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(resid_crypto)\n",
    "ax.axhline(mu_crypto,color = 'orange',label = \"mu\")\n",
    "ax.axhline(mu_crypto+sig_crypto,color = 'red',linestyle ='--',label = \"mu+sigma\")\n",
    "ax.axhline(mu_crypto-sig_crypto,color = 'green',linestyle ='--',label = \"mu-sigma\")\n",
    "ax.axvline(prices_train_crypto.index[-1],color = 'black',linestyle ='--',label = \"Time series split\")\n",
    "ax.set_title(\"(BNB-USD, ETH-USD) Trading Signal\")\n",
    "ax.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part II: Backtesting\n",
    "In this section, we are going to implement the following steps:\n",
    "\n",
    "- Perform systematic backtesting of your trading strategy platform to produce drawdown plots, rolling Sharpe ratio and rolling beta\n",
    "- keep delivering staionary spread over 3-6 months. Re-estmate cointergration by shifting data 1-2 weeks and report beta and EG.\n",
    "- Use Machine-learning-inspired backtesting, such as spliting data, time series CV.\n",
    "\n",
    "## 2.1 Pairs trading backtesting\n",
    "I have designed a pairs trading backtesting function below to iterate every time point and implement the predetermined strategy. This function will return a trackable trading dataframe and various visualized trading parameters, including cumulative returns, residual with signal, pair prices, maximum drawdown, 6-month rolling Sharpe ratio and 6-month rolling volatility."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "import matplotlib.dates as mdates\n",
    "def pairs_trading_backtesting(prices, mu, sigma, hedge_ratio, Z=1, money_init=10000, bet_size=0.2,bid_ask_spread=0.0,commission_cost_pct=0.0, verbose=False, plot=True):\n",
    "    '''\n",
    "    pair_prices: dataframe, first column is Y, second column will be X\n",
    "    mu: float, the average mean reversion level\n",
    "    sigma: float, the volatility of the residual series\n",
    "    Z: float, the multiple to adjust trading threshold\n",
    "    money_init: float, the initial money setting\n",
    "    bet_size: float, the proportion of cash to invest for each trade\n",
    "    bid_ask_spread: float, the absolute value of bid ask spread\n",
    "    commission_cost_pct: float, the percentage of commission cost for each trade\n",
    "    verbose: bool, print each transaction information or not\n",
    "    plot: bool, print strategy details in figures or not\n",
    "    '''\n",
    "    global Y_shares\n",
    "    global X_shares\n",
    "    global Y_assetValue\n",
    "    global X_assetValue\n",
    "    global Cash\n",
    "    global t\n",
    "    initial = money_init\n",
    "#     Z=1\n",
    "    ## Set the leverage for the percentage of money in each transaction. \n",
    "    ## If bet_size>1, we borrow money and have a leverage.\n",
    "    ## If bet_size<1, we partially invest and hold a percentage of cash.\n",
    "\n",
    "    pair_prices = prices.copy()\n",
    "    Y = pair_prices.iloc[:,0]\n",
    "    X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "    _, residuals, _ = OLS(Y,X)\n",
    "    mu = mu\n",
    "    sigma = sigma\n",
    "\n",
    "    pair_prices['beta_coint'] = hedge_ratio\n",
    "    pair_prices['resid'] = residuals\n",
    "\n",
    "\n",
    "    pair_prices['normalized resid'] = (residuals-mu)/sigma\n",
    "\n",
    "    open_signal = 0\n",
    "    close_signal = 0\n",
    "    hold_position = 0\n",
    "    pair_prices\n",
    "\n",
    "    # Cash\n",
    "    Cash = initial\n",
    "    # Asset\n",
    "    Y_shares = 0\n",
    "    X_shares = 0\n",
    "    Y_assetValue = 0\n",
    "    X_assetValue = 0\n",
    "    PortfolioValue = Y_assetValue+X_assetValue\n",
    "    # Equity\n",
    "    Equity = Cash + PortfolioValue\n",
    "    Trade_n = 0\n",
    "    \n",
    "    pair_prices['Cash'] = np.nan\n",
    "    pair_prices[f'{Y.name}_shares'],pair_prices[f'{X.name}_shares'] = np.nan,np.nan\n",
    "    pair_prices[f'{Y.name}_values'],pair_prices[f'{X.name}_values'] = np.nan,np.nan\n",
    "    pair_prices['PortfolioValue'],pair_prices['Equity'] = np.nan,np.nan\n",
    "    pair_prices['#Trade'] = np.nan\n",
    "    def place_order(contract, action, quantity, verbose=verbose):\n",
    "        '''\n",
    "        place order for pairs trading\n",
    "        @params contract: 'X' or 'Y', trade asset X or Y\n",
    "        @params action: int, 1 or -1, long or short the asset\n",
    "        @params quantity: the quantitiy of placing orders\n",
    "        '''\n",
    "        global Y_shares\n",
    "        global X_shares\n",
    "        global Y_assetValue\n",
    "        global X_assetValue\n",
    "        global Cash\n",
    "        global t\n",
    "        if contract=='Y':\n",
    "            Y_shares += action * quantity\n",
    "\n",
    "            ## bid ask price correction\n",
    "            if action==1:\n",
    "                ## buy asset at ask price\n",
    "                orderValue = action * quantity * (Y_price_t+bid_ask_spread)\n",
    "            elif action==-1:\n",
    "                ## buy asset at ask price\n",
    "                orderValue = action * quantity * (Y_price_t-bid_ask_spread)\n",
    "\n",
    "            cost = abs(orderValue*commission_cost_pct)\n",
    "\n",
    "            Cash -= (orderValue + cost)\n",
    "            Y_assetValue += orderValue\n",
    "            if verbose:\n",
    "                print(f\"{t}: order placed! we ordered {action *quantity} numbers of {Y.name} at price {Y_price_t}.\")\n",
    "\n",
    "        if contract=='X':\n",
    "            X_shares += action * quantity\n",
    "\n",
    "            ## bid ask price correction\n",
    "            if action==1:\n",
    "                ## buy asset at ask price\n",
    "                orderValue = action * quantity * (X_price_t+bid_ask_spread)\n",
    "            elif action==-1:\n",
    "                ## buy asset at ask price\n",
    "                orderValue = action * quantity * (X_price_t-bid_ask_spread)\n",
    "\n",
    "\n",
    "            cost = abs(orderValue*commission_cost_pct)\n",
    "\n",
    "            Cash -= (orderValue + cost)\n",
    "            X_assetValue += orderValue\n",
    "            if verbose:\n",
    "                print(f\"{t}: order placed! we ordered {action *quantity} numbers of {X.name} at price {X_price_t}.\")\n",
    "    \n",
    "    \n",
    "    for t in pair_prices.index:\n",
    "        trade_size = Cash * bet_size\n",
    "        ## Before the market: nothing to do yet\n",
    "\n",
    "        ## During the market: we calculate the indicator, and place order or just hold\n",
    "        Y_price_t =  pair_prices.loc[t,Y.name]\n",
    "        X_price_t =  pair_prices.loc[t,X.name]\n",
    "        beta = pair_prices.loc[t,'beta_coint']\n",
    "        spread = Y_price_t - beta * X_price_t\n",
    "        order_number = abs(trade_size/(Y_price_t + beta * X_price_t))\n",
    "    #     print(\"order:\",order_number)\n",
    "        if pair_prices.loc[t,'normalized resid']>=Z and hold_position==0:\n",
    "            ### if open signal ==1, which means e_t = Y-𝛽X-𝛼 beyond the upper threshold\n",
    "            ### We go short Y and long X to expect the e_t going down\n",
    "            ## place_order(contract, action, quantity)\n",
    "            place_order('Y', action = -1, quantity = order_number)\n",
    "            place_order('X', action = 1, quantity = order_number * beta)\n",
    "            hold_position = -1\n",
    "            Trade_n+=1\n",
    "\n",
    "        if pair_prices.loc[t,'normalized resid']<=-Z and hold_position==0:\n",
    "            ### if open signal ==-1, which means e_t = Y-𝛽X-𝛼 less than the lower threshold\n",
    "            ### We go long Y and short X to expect the e_t going up\n",
    "            ## place_order(contract, action, quantity)\n",
    "            place_order('Y', action = 1, quantity = order_number)\n",
    "            place_order('X', action = -1, quantity = order_number * beta)\n",
    "            hold_position = 1\n",
    "            Trade_n+=1\n",
    "            \n",
    "        if hold_position==-1 and pair_prices.loc[t,'normalized resid']<=0:\n",
    "            ## close position\n",
    "            place_order('Y', action = 1, quantity = abs(Y_shares))\n",
    "            place_order('X', action = -1, quantity = abs(X_shares))\n",
    "            hold_position = 0\n",
    "\n",
    "        if hold_position==1 and pair_prices.loc[t,'normalized resid']>=0:\n",
    "            ## close position\n",
    "            place_order('Y', action = -1, quantity = abs(Y_shares))\n",
    "            place_order('X', action = 1, quantity = abs(X_shares))\n",
    "            hold_position = 0\n",
    "        ## after the market: recalculate the daily PnL, returns, market value, equity, etc.\n",
    "        Y_assetValue = Y_shares * Y_price_t\n",
    "        X_assetValue = X_shares * X_price_t\n",
    "        PortfolioValue = (Y_assetValue+X_assetValue)\n",
    "        Equity = Cash + PortfolioValue\n",
    "        pair_prices.loc[t,'Cash'], pair_prices.loc[t,'PortfolioValue'], pair_prices.loc[t,'Equity'] = Cash, PortfolioValue, Equity\n",
    "        pair_prices.loc[t,f'{Y.name}_shares'], pair_prices.loc[t,f'{X.name}_shares'] =  Y_shares, X_shares\n",
    "        pair_prices.loc[t,f'{Y.name}_values'], pair_prices.loc[t,f'{X.name}_values'] =  Y_assetValue, X_assetValue\n",
    "        pair_prices.loc[t,'#Trade'] = Trade_n\n",
    "#         print(t,Cash, Y_assetValue, X_assetValue,PortfolioValue,Equity)\n",
    "    pair_prices['return'] = pair_prices['Equity'].pct_change()\n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    # Draw Plots#\n",
    "    #############\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(3,2,figsize = (18,18))\n",
    "        ax = axes[0][0]\n",
    "        ax.plot(pair_prices['Equity']/initial-1)\n",
    "        ax.set_title('P&L(Cumulative return)',fontsize=14)\n",
    "\n",
    "\n",
    "        ax = axes[0][1]\n",
    "        ax.plot(pair_prices['resid'])\n",
    "        ax.axhline(mu,color = 'orange',label = \"mu\")\n",
    "        ax.axhline(mu+Z*sigma,color = 'red',linestyle ='--',label = \"upper threshold\")\n",
    "        ax.axhline(mu-Z*sigma,color = 'green',linestyle ='--',label = \"lower threshold\")\n",
    "        ax.set_title(\"Residuals with trading signals\",fontsize=14)\n",
    "        ax.legend()\n",
    "\n",
    "        ax = axes[1][0]\n",
    "        ax.plot(pair_prices.iloc[:,0], label = pair_prices.columns[0])\n",
    "        ax.plot(pair_prices.iloc[:,1]*pair_prices.beta_coint, label = \"Beta * \"+pair_prices.columns[1])\n",
    "        ax.set_title(\"Prices with trading signals\",fontsize=14)\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "        ax = axes[1][1]\n",
    "        rolling_max = pair_prices['Equity'].expanding().max()\n",
    "        max_drawdown = pair_prices['Equity']/rolling_max-1\n",
    "        ax.plot(max_drawdown)\n",
    "        ax.set_title(\"Maximum Drawdown\",fontsize=14)\n",
    "\n",
    "\n",
    "        ax = axes[2][0]\n",
    "        rolling_SR = pair_prices['return'].rolling(20).apply(lambda x: (x.mean()) / x.std(), raw = True)\n",
    "        rolling_SR.ffill(inplace=True)\n",
    "        rolling_SR.plot(ax = ax,lw=2, color='orange',  label='Sharpe').axhline(y = rolling_SR.mean(), color = \"blue\", lw = 2,\n",
    "                                                         linestyle = '--',label = 'Average')\n",
    "\n",
    "        ax.set_title(\"Rolling Sharpe Ratio (1-month)\",fontsize=14)\n",
    "        ax.legend()\n",
    "  \n",
    "\n",
    "        ax = axes[2][1]\n",
    "        rolling_vol = pair_prices['return'].rolling(20).std()*np.sqrt(252)\n",
    "        rolling_vol.ffill(inplace=True)\n",
    "        rolling_vol.plot(ax = ax,lw=2, color='orange',  label='Vol').axhline(y =rolling_vol.mean(), color = \"blue\", lw = 2,\n",
    "                                                         linestyle = '--',label = 'Average')\n",
    "\n",
    "        ax.set_title(\"Rolling Annualised Volatility (1-month)\",fontsize=14)\n",
    "        ax.legend()\n",
    "    \n",
    "        ##################\n",
    "        # Trading Outcome#\n",
    "        ##################\n",
    "        print(f\"Start date:\", pair_prices.index[0])\n",
    "        print(f\"End date:\", pair_prices.index[-1])\n",
    "        print(f\"Net Profit: {pair_prices['Equity'][-1]-initial:.2f}\")\n",
    "        print(f\"Total return: {(pair_prices['Equity'][-1]/initial - 1)*100:.2f}%\")\n",
    "        \n",
    "\n",
    "\n",
    "    return pair_prices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1 Crude oil-gasoline backtesting on the training set (Z=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_commodity = prices_train_commodity\n",
    "pair_prices = pair_commodity.copy()\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']\n",
    "trade_df = pairs_trading_backtesting(pair_commodity, \n",
    "                                      mu = mu, sigma = sigma, \n",
    "                                      hedge_ratio = beta, \n",
    "                                      Z=1, \n",
    "                                      money_init=10000, \n",
    "                                      bet_size=1,\n",
    "                                      bid_ask_spread=0.0,\n",
    "                                      commission_cost_pct=0.0,\n",
    "                                      verbose=False,\n",
    "                                      plot=True)\n",
    "# trade_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 2.1.2 Backtesting for BNB-ETH pair on training set (Z=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_crypto = prices_train_crypto\n",
    "pair_prices = pair_crypto.copy()\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']\n",
    "trade_df = pairs_trading_backtesting(pair_crypto, \n",
    "                                      mu = mu, sigma = sigma, \n",
    "                                      hedge_ratio = beta, \n",
    "                                      Z=1, \n",
    "                                      money_init=10000, \n",
    "                                      bet_size=1,\n",
    "                                      bid_ask_spread=0.0,\n",
    "                                      commission_cost_pct=0.0,\n",
    "                                      verbose=False,\n",
    "                                      plot=True)\n",
    "# trade_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 2.2 Backtesting params optimization\n",
    "At first, with Z=1, we got the backtest result. But it may not be the best entry signal. We plan to vary Z up and down to calculate the highest gain/loss for each threshold. But this may lead to overfitting problem, so if we optimize only on the training set and apply the best value on the test set, the overfitting problem can be solved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### $Z_{opt}$ optimization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_optimized_Z(pair_prices, start, end, n_steps, mu, sigma, beta):\n",
    "\n",
    "    \n",
    "    pair_prices = pair_prices.copy()\n",
    "    Y = pair_prices.iloc[:,0]\n",
    "    X = pair_prices.iloc[:,1]\n",
    "    res = pd.DataFrame(columns = ['PnL','#Trade'])\n",
    "\n",
    "    for z in np.linspace(start, end, n_steps):\n",
    "        df = pairs_trading_backtesting(pair_prices, \n",
    "                                              mu = mu, sigma = sigma, \n",
    "                                              hedge_ratio = beta, \n",
    "                                              Z=z, \n",
    "                                              money_init=10000, \n",
    "                                              bet_size=1,\n",
    "                                              bid_ask_spread=0.0,\n",
    "                                              commission_cost_pct=0.0,\n",
    "                                              verbose=False,\n",
    "                                              plot=False)\n",
    "        res.loc[z] = [df['Equity'].iloc[-1]/10000-1, df['#Trade'].iloc[-1]]\n",
    "\n",
    "        \n",
    "    Z_opt = res['PnL'].idxmax()\n",
    "    print(f\"Z_opt = {Z_opt:.2f} is the best performance parameter with {100*res['PnL'].max():.2f}% cum return\")\n",
    "    fig, axes = plt.subplots(1,2,figsize=(10,5))\n",
    "    ax = axes[0]\n",
    "    ax.plot(res['PnL'],label = 'cum return')\n",
    "    ax.set_title(\"PnL for each Z\")\n",
    "    ax.set_xlabel('Z')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(res['#Trade'])\n",
    "    ax.set_title(\"Number of Trade\")\n",
    "    ax.set_xlabel('Z')\n",
    "    return Z_opt, res\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pair_prices = prices_train_commodity\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']    \n",
    "\n",
    "Z_opt, report = find_optimized_Z(pair_prices, 0.5,2,16,mu, sigma, beta)\n",
    "# report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pair_prices = prices_train_crypto\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']    \n",
    "\n",
    "\n",
    "Z_opt, report = find_optimized_Z(pair_prices, 0.1,2,20,mu, sigma, beta)\n",
    "# report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the simulating Z from 0.5 to 2, the optimum parameter Z for both commodity pair and crypto pair is 1.0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Optimal $Z_{opt}$ applied to test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 Commodity pair backtesting on test set \n",
    "Due to the big crash during the 2020 Covid, the maximum drawdown in pairs trading strategies was greatly affected. Therefore, even if we have cointegrated spread to trade, we need to be aware of the market risks. We earned 74.92% return from 2017 to 2022.\n",
    "\n",
    "Otherwise, our pairs trading strategy worked well through almost all the time period."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_commodity = prices_train_commodity\n",
    "pair_prices = pair_commodity.copy()\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']\n",
    "trade_df = pairs_trading_backtesting(prices_test_commodity, \n",
    "                                      mu = mu, sigma = sigma, \n",
    "                                      hedge_ratio = beta, \n",
    "                                      Z=1, \n",
    "                                      money_init=10000, \n",
    "                                      bet_size=1,\n",
    "                                      bid_ask_spread=0.0,\n",
    "                                      commission_cost_pct=0.0,\n",
    "                                      verbose=False,\n",
    "                                      plot=True)\n",
    "# trade_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.2. Crypto pair backtesting on test set\n",
    "Although we don't have enough trade during test dataset, we still got a successful one trade in 2022, and received 8% return with $801.42 net profit."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_crypto = prices_train_crypto\n",
    "pair_prices = pair_crypto.copy()\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "\n",
    "\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']\n",
    "trade_df = pairs_trading_backtesting(prices_test_crypto, \n",
    "                                      mu = mu, sigma = sigma, \n",
    "                                      hedge_ratio = beta, \n",
    "                                      Z=1, \n",
    "                                      money_init=10000, \n",
    "                                      bet_size=1,\n",
    "                                      bid_ask_spread=0.0,\n",
    "                                      commission_cost_pct=0.0,\n",
    "                                      verbose=False,\n",
    "                                      plot=True)\n",
    "# trade_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Bid-Ask Spread impact\n",
    "Bid-ask spreads are critical to investors and represent the liquidity of a given asset. The bid-ask spread can affect the price at which a purchase or sale is made, which in turn affects the investor's overall portfolio return.\n",
    "From our analysis below, I tested how bid-ask spreads can have an impact on the total return of a strategy. I set the spreads from 0 to 0.05 with an interval of 0.01. It turns out that as the spreads get larger, the PnL get smaller. When the spread was set to 0.05, the total return was less than half of what it would have been had we not taken into account any spreads."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_prices = price_commodity[['Crude Oil','Gasoline']]\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']    \n",
    "\n",
    "res = pd.DataFrame(columns = ['PnL','#Trade'])\n",
    "\n",
    "for p in np.linspace(0., 0.05, 20):\n",
    "    df = pairs_trading_backtesting(pair_prices, \n",
    "                                          mu = mu, sigma = sigma, \n",
    "                                          hedge_ratio = beta, \n",
    "                                          Z=1, \n",
    "                                          money_init=10000, \n",
    "                                          bet_size=1,\n",
    "                                          bid_ask_spread=p,\n",
    "                                          commission_cost_pct=0.0,\n",
    "                                          verbose=False,\n",
    "                                          plot=False)\n",
    "    res.loc[p] = [df['Equity'].iloc[-1]/10000-1, df['#Trade'].iloc[-1]]\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(res['PnL'],label = 'cum return')\n",
    "ax.set_title(\"PnL for each Bid-Ask spread pct\")\n",
    "ax.set_xlabel('bid-ask spread pct')\n",
    "ax.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 transaction costs Impact\n",
    "Transaction fees are another cost in running strategy. It typically is a percentage of the total trade value. When we tuned from 0% to 5% with an interval of 0.1%, our trading revenue drops exponentially, which is so important that we can't ignore the part of cost."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pair_prices = price_commodity[['Crude Oil','Gasoline']]\n",
    "Y = pair_prices.iloc[:,0]\n",
    "X = pair_prices.iloc[:,1]\n",
    "beta, residuals, _ = OLS(Y,X)\n",
    "OU_params = OU_process().fit(residuals,verbose=False).params\n",
    "mu = OU_params['theta']\n",
    "sigma = OU_params['sigma_eq']    \n",
    "\n",
    "res = pd.DataFrame(columns = ['PnL','#Trade'])\n",
    "\n",
    "for p in np.linspace(0., 0.05, 20):\n",
    "    df = pairs_trading_backtesting(pair_prices, \n",
    "                                          mu = mu, sigma = sigma, \n",
    "                                          hedge_ratio = beta, \n",
    "                                          Z=1, \n",
    "                                          money_init=10000, \n",
    "                                          bet_size=1,\n",
    "                                          bid_ask_spread=0.0,\n",
    "                                          commission_cost_pct=p,\n",
    "                                          verbose=False,\n",
    "                                          plot=False)\n",
    "    res.loc[p] = [df['Equity'].iloc[-1]/10000-1, df['#Trade'].iloc[-1]]\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(res['PnL'],label = 'cum return')\n",
    "ax.set_title(\"PnL for each commission_cost_pct\")\n",
    "ax.set_xlabel('commission cost for each trade')\n",
    "ax.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reference:\n",
    "[1] https://www.sciencedirect.com/science/article/abs/pii/S1386418114000809\n",
    "\n",
    "[2] https://github.com/cantaro86/Financial-Models-Numerical-Methods/blob/master/6.1%20Ornstein-Uhlenbeck%20process%20and%20applications.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
